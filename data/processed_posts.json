[
    {
        "text": "I uploaded 128K context 8bit, 6, 4, 3, and 2bit quants for Qwen 2.5 Coder 32B which matches GPT4o / Claude 3.5 Sonnet performance in coding tasks! Uploads are at: https://lnkd.in/gGKC6pyw\n\nThe 4bit quant Q4_K_M is recommended - it uses ~20 to 24GB of VRAM / RAM, so if your machine has a combined total of 24GB (RTX 4090 works), it'll work well!\n\nThe 2bit quant Q2_K uses 12 to 15GB, so smaller GPUs also can fit - there is some performance degradation though.",
        "engagement": {
            "likes": 386,
            "comments": 20,
            "reposts": 24
        }
    },
    {
        "text": "Qwen 2.5 Coder 32B just matched GPT4o's performance on coding! And it's free! You can finetune 14B for free on Colab with Unsloth AI 2x faster & using 60% less VRAM with no accuracy degradation.\n\nGoogle Colab 14B notebook: https://lnkd.in/g6v_MFtP\nKaggle 14B notebook: https://lnkd.in/g9R9vbMA",
        "engagement": {
            "likes": 598,
            "comments": 45,
            "reposts": 44
        }
    },
    {
        "text": "If you haven't read a book in a while, I highly recommend The LLM Engineer's Handbook by Maxime Labonne and Paul Iusztin.\n\nThe book guides you through LLM fundamentals, complete with helpful code examples and visuals, so you can use LLMs to its fullest potential.",
        "engagement": {
            "likes": 542,
            "comments": 27,
            "reposts": 24
        }
    },
    {
        "text": "If you're on Python 3.10, switch to 3.11! Linux machines are ~1.25x faster. Mac 1.2x faster. Windows 1.12x. Unsloth AI will be faster as well! But why is 3.11 faster?",
        "engagement": {
            "likes": 653,
            "comments": 33,
            "reposts": 41
        }
    },
    {
        "text": "Fine-tuning LLMs with Unsloth AI is fantastic! It reduced my training costs by >78.62%. This is how ‚Üì\n\nWhile writing the code for the LLM Engineer's Handbook, we first did fine-tuning solely using `ùòµùò≥ùò≠` from Hugging Face.",
        "engagement": {
            "likes": 367,
            "comments": 24,
            "reposts": 34
        }
    },
    {
        "text": "We're currently at the GitHub Universe so feel free to stop by to say hi! ü¶•‚ô•Ô∏è\n\nWe'll be handing out Unsloth AI stickers, merch and more. Also if you want to have a chat about AI or Unsloth we'd love to!\n\nWe're in the Open Source Zone at the Gateway Pavillon in the Fort Mason Center.",
        "engagement": {
            "likes": 552,
            "comments": 22,
            "reposts": 6
        }
    },
    {
        "text": "Sharing 2 free notebooks to finetune Llama 3.2 with ü¶•Unsloth AI! They also include our gradient accumulation bug fix which affected all training runs in AI!",
        "engagement": {
            "likes": 335,
            "comments": 26,
            "reposts": 18
        }
    },
    {
        "text": "We'll be at GitHub Universe in SF next week Tuesday! If you want some cute ü¶•Unsloth AI stickers or merch, come say hi at our stand!",
        "engagement": {
            "likes": 202,
            "comments": 9,
            "reposts": 3
        }
    },
    {
        "text": "A transformer's depth affects its reasoning capabilities, whilst model size affects its knowledge capacity. Experiments show wider LLMs don't affect reasoning capabilities but more depth (more layers) helps increase it.",
        "engagement": {
            "likes": 329,
            "comments": 15,
            "reposts": 27
        }
    },
    {
        "text": "I'm super excited to announce that Unsloth AI is part of Y Combinator S24! We‚Äôre so pumped to bring fine-tuning to an even larger audience and showcase how easy it can be to build your own custom ChatGPT!",
        "engagement": {
            "likes": 1326,
            "comments": 211,
            "reposts": 18
        }
    },
    {
        "text": "Mistral AI just dropped a new vision multimodal model called Pixtral 12b! I also downloaded params.json - GeLU & 2D RoPE are used for the vision adapter. The vocab size also got larger - 131072. Also, Mistral's latest tokenizer PR shows 3 extra new tokens (the image, the start & end).",
        "engagement": {
            "likes": 372,
            "comments": 7,
            "reposts": 18
        }
    },
    {
        "text": "Unsloth AIü¶• just hit 2 million monthly downloads on Hugging Face today! ü•≥ We make LLM fine-tuning 2x faster and use 70% less memory with no accuracy degradation!",
        "engagement": {
            "likes": 847,
            "comments": 86,
            "reposts": 22
        }
    },
    {
        "text": "Did you know ü¶•Unsloth AI has docs now? If you have ideas on improving it, I'm all ears! There's 30 free Colab & Kaggle notebooks for faster fine-tuning & inference listed on one page!",
        "engagement": {
            "likes": 303,
            "comments": 16,
            "reposts": 15
        }
    },
    {
        "text": "The State of AI Report 2024 is out! With over 200 pages of information about AI, it's definitely worth a read! Unsloth AI also got a mention on page 139. Report link: www.stateof.ai",
        "engagement": {
            "likes": 257,
            "comments": 24,
            "reposts": 8
        }
    },
    {
        "text": "We're releasing 2 free notebooks for 2x faster & 60% less memory fine-tuning for Mistral NeMo 12b! Mistral's latest free LLM is the largest multilingual OSS model which can fit in a free Colab GPU!",
        "engagement": {
            "likes": 571,
            "comments": 36,
            "reposts": 46
        }
    }
]
